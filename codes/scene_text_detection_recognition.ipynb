from google.colab import drive
import cv2
import numpy as np
import string
import matplotlib.pyplot as plt
from imutils.object_detection import non_max_suppression
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Lambda, Bidirectional, LSTM, Dense
import os

# Mount Google Drive
drive.mount('/content/drive')

# Character list for recognition
char_list = string.ascii_letters + string.digits

# Path configuration
CRNN_MODEL_PATH = "/content/drive/MyDrive/OCR_Project/models/crnn_model.h5"
DEMO_IMAGE_PATH = "/content/drive/MyDrive/OCR_Project/test_images/demo.jpeg"
OUTPUT_DIR = "/content/drive/MyDrive/OCR_Project/outputs"

def display_intro():
    print("""
    =============================================
    Optical Character Recognition (OCR) System
    =============================================

    This system demonstrates:
    1. Text detection using EAST (OpenCV implementation)
    2. Text recognition using CRNN
    """)

def load_east_model():
    """Load the fallback OpenCV EAST model"""
    if not os.path.exists("/tmp/frozen_east_text_detection.pb"):
        east_url = "https://www.dropbox.com/s/r2ingd0l3zt8hxs/frozen_east_text_detection.tar.gz?dl=1"
        os.system("wget " + east_url + " -O /tmp/east_model.tar.gz")
        os.system("tar -xzf /tmp/east_model.tar.gz -C /tmp/")
    return cv2.dnn.readNet("/tmp/frozen_east_text_detection.pb")

def load_and_preprocess_image(image_path):
    """Load and preprocess image"""
    img = cv2.imread(image_path)
    if img is None:
        raise ValueError(f"Image not found at {image_path}")
    return img

def detect_text(image):
    """Detect text regions using EAST"""
    (h, w) = image.shape[:2]
    rW = w / float(320)
    rH = h / float(320)

    # Resize and preprocess for EAST
    blob = cv2.dnn.blobFromImage(image, 1.0, (320, 320),
                               (123.68, 116.78, 103.94), swapRB=True, crop=False)

    # Load EAST detector
    net = load_east_model()
    net.setInput(blob)
    (scores, geometry) = net.forward(["feature_fusion/Conv_7/Sigmoid",
                                    "feature_fusion/concat_3"])

    # Decode predictions
    (rects, confidences) = decode_predictions(scores, geometry)
    boxes = non_max_suppression(np.array(rects), probs=confidences)

    # Scale boxes back to original image size
    results = []
    for (startX, startY, endX, endY) in boxes:
        startX = int(startX * rW)
        startY = int(startY * rH)
        endX = int(endX * rW)
        endY = int(endY * rH)
        results.append((startX, startY, endX, endY))

    return results

def decode_predictions(scores, geometry):
    """Helper function to decode EAST predictions"""
    (numRows, numCols) = scores.shape[2:4]
    rects = []
    confidences = []

    for y in range(0, numRows):
        scoresData = scores[0, 0, y]
        xData0 = geometry[0, 0, y]
        xData1 = geometry[0, 1, y]
        xData2 = geometry[0, 2, y]
        xData3 = geometry[0, 3, y]
        anglesData = geometry[0, 4, y]

        for x in range(0, numCols):
            if scoresData[x] < 0.5:
                continue

            (offsetX, offsetY) = (x * 4.0, y * 4.0)
            angle = anglesData[x]
            cos = np.cos(angle)
            sin = np.sin(angle)

            h = xData0[x] + xData2[x]
            w = xData1[x] + xData3[x]

            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
            startX = int(endX - w)
            startY = int(endY - h)

            rects.append((startX, startY, endX, endY))
            confidences.append(scoresData[x])

    return (rects, confidences)

def preprocess_for_crnn(roi):
    """Preprocess text region for CRNN"""
    img = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    (h, w) = img.shape

    # Resize maintaining aspect ratio
    if w > 128:
        img = cv2.resize(img, (128, h))
    if h > 32:
        img = cv2.resize(img, (128, 32))

    # Pad if needed
    (h, w) = img.shape
    if h < 32:
        img = np.pad(img, [(0, 32-h), (0,0)], mode='constant', constant_values=255)
    if w < 128:
        img = np.pad(img, [(0,0), (0, 128-w)], mode='constant', constant_values=255)

    img = np.expand_dims(img, axis=-1)
    return img / 255.0

def build_crnn_model():
    """Build CRNN model architecture with proper output shapes"""
    inputs = Input(shape=(32,128,1))

    # Convolutional layers
    x = Conv2D(64, (3,3), activation='relu', padding='same')(inputs)
    x = MaxPool2D(pool_size=(2,2), strides=2)(x)

    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = MaxPool2D(pool_size=(2,2), strides=2)(x)

    x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
    x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
    x = MaxPool2D(pool_size=(2,1))(x)

    x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(pool_size=(2,1))(x)

    x = Conv2D(512, (2,2), activation='relu')(x)

    # Proper Lambda layer with output shape
    x = Lambda(lambda x: tf.squeeze(x, axis=1), output_shape=(31, 512))(x)

    # LSTM layers
    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)
    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)

    # Output layer
    outputs = Dense(len(char_list)+1, activation='softmax')(x)

    return Model(inputs, outputs)

def load_crnn_model():
    """Load CRNN model with proper architecture"""
    try:
        model = build_crnn_model()
        model.load_weights(CRNN_MODEL_PATH)
        return model
    except Exception as e:
        print(f"Error loading CRNN model: {str(e)}")
        return None

def recognize_text(model, cropped_images):
    """Recognize text using the model"""
    if model is None:
        return [""] * len(cropped_images)

    # Prepare batch
    batch = np.array([preprocess_for_crnn(img) for img in cropped_images])

    try:
        # Predict
        preds = model.predict(batch, verbose=0)

        # Decode predictions
        results = []
        input_length = np.ones(preds.shape[0]) * preds.shape[1]

        # Process each prediction individually to avoid shape issues
        for i in range(preds.shape[0]):
            pred = np.expand_dims(preds[i], axis=0)
            decoded = tf.keras.backend.ctc_decode(
                pred,
                input_length=np.array([pred.shape[1]]),
                greedy=True
            )[0][0].numpy()

            text = ''.join([char_list[int(x)] for x in decoded[0] if int(x) != -1])
            results.append(text)

        return results
    except Exception as e:
        print(f"Error during recognition: {str(e)}")
        return [""] * len(cropped_images)

def visualize_results(image, boxes, texts):
    """Display and save results"""
    output = image.copy()

    # Draw bounding boxes and text
    for i, (box, text) in enumerate(zip(boxes, texts)):
        (startX, startY, endX, endY) = box
        cv2.rectangle(output, (startX, startY), (endX, endY), (0, 255, 0), 2)

        # Add recognized text
        if text:
            cv2.putText(output, text, (startX, startY-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        else:
            cv2.putText(output, f"Region {i+1}", (startX, startY-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

    # Display
    plt.figure(figsize=(15, 10))
    plt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    # Save
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    cv2.imwrite(os.path.join(OUTPUT_DIR, "final_output.jpg"), output)

def main():
    display_intro()

    try:
        # Verify files
        if not os.path.exists(DEMO_IMAGE_PATH):
            raise FileNotFoundError(f"Demo image not found at {DEMO_IMAGE_PATH}")

        # Load image
        print("Loading image...")
        image = load_and_preprocess_image(DEMO_IMAGE_PATH)

        # Detect text
        print("Detecting text regions...")
        boxes = detect_text(image)
        print(f"Found {len(boxes)} text regions")

        # Crop detected regions
        cropped_images = [image[startY:endY, startX:endX] for (startX, startY, endX, endY) in boxes]

        # Load CRNN model
        print("Loading text recognizer...")
        crnn_model = load_crnn_model()

        # Recognize text
        print("Recognizing text...")
        recognized_texts = recognize_text(crnn_model, cropped_images)

        # Display results
        visualize_results(image, boxes, recognized_texts)

        print("\nProcessing complete! Results saved in:", OUTPUT_DIR)
        print("\nText regions with recognized text:")
        for i, ((startX, startY, endX, endY), text) in enumerate(zip(boxes, recognized_texts)):
            print(f"Region {i+1}: ({startX}, {startY}) to ({endX}, {endY}) - {text if text else 'Not recognized'}")

    except Exception as e:
        print(f"\nError: {str(e)}")
        print("\nTroubleshooting steps:")
        print("1. Check all file paths are correct")
        print("2. Verify your image file exists")
        print("3. Ensure you have internet access for EAST model download")

if __name__ == "__main__":
    main()

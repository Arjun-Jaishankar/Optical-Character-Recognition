import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

corpus = [
    "Optical Character Recognition is the process of extracting text from images.",
    "Bounding boxes help in localizing text regions within an image.",
    "Text detection involves identifying regions in an image where text is present.",
    "Text recognition plays a crucial role in automating data entry for digitizing handwritten and printed documents.",
    "Real-time text recognition is employed in mobile applications to scan and translate text instantly.",
    "Recognition of handwritten text, a specific category of text recognition, demands advanced techniques for sequence modeling."
]

def preprocess_corpus(corpus):
    tokenized_sentences = []
    vocab = set()
    for sentence in corpus:
        words = sentence.lower().split()
        tokenized_sentences.append(words)
        vocab.update(words)
    vocab = sorted(vocab)
    word_to_index = {word: i for i, word in enumerate(vocab)}
    index_to_word = {i: word for word, i in word_to_index.items()}
    return tokenized_sentences, word_to_index, index_to_word

tokenized_sentences, word_to_index, index_to_word = preprocess_corpus(corpus)
vocab_size = len(word_to_index)
embedding_dim = 10

def generate_skipgram_data(tokenized_sentences, word_to_index, window_size=2):
    training_data = []
    for sentence in tokenized_sentences:
        for i, target_word in enumerate(sentence):
            target_index = word_to_index[target_word]
            context_indices = list(range(max(0, i - window_size), min(len(sentence), i + window_size + 1)))
            context_indices.remove(i)
            for context_index in context_indices:
                context_word = sentence[context_index]
                training_data.append((target_index, word_to_index[context_word]))
    return training_data

def generate_cbow_data(tokenized_sentences, word_to_index, window_size=2):
    training_data = []
    for sentence in tokenized_sentences:
        for i, target_word in enumerate(sentence):
            target_index = word_to_index[target_word]
            context_indices = list(range(max(0, i - window_size), min(len(sentence), i + window_size + 1)))
            context_indices.remove(i)
            context_words = [sentence[idx] for idx in context_indices]
            training_data.append((context_words, target_index))
    return training_data

skipgram_data = generate_skipgram_data(tokenized_sentences, word_to_index)
cbow_data = generate_cbow_data(tokenized_sentences, word_to_index)

class CBOW(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, context):
        embedded = self.embeddings(context).mean(dim=1)
        output = self.linear(embedded)
        return output

class SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, target):
        embedded = self.embeddings(target)
        output = self.linear(embedded)
        return output

def train_model(model, data, epochs, learning_rate, is_cbow=True):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(data)
        for context, target in data:
            optimizer.zero_grad()
            if is_cbow:
                context_idxs = torch.tensor([[word_to_index[word] for word in context]], dtype=torch.long)
                target_idx = torch.tensor([target], dtype=torch.long)
                output = model(context_idxs)
            else:
                target_idx = torch.tensor([context], dtype=torch.long)
                output = model(target_idx)

            loss = criterion(output, target_idx)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}")

cbow_model = CBOW(vocab_size, embedding_dim)
skipgram_model = SkipGram(vocab_size, embedding_dim)

print("Training CBOW Model")
train_model(cbow_model, cbow_data, epochs=20, learning_rate=0.01, is_cbow=True)

print("Training Skip-gram Model")
train_model(skipgram_model, skipgram_data, epochs=20, learning_rate=0.01, is_cbow=False)

cbow_embeddings = cbow_model.embeddings.weight.detach().numpy()
skipgram_embeddings = skipgram_model.embeddings.weight.detach().numpy()

pca = PCA(n_components=2)
cbow_2d = pca.fit_transform(cbow_embeddings)
skipgram_2d = pca.fit_transform(skipgram_embeddings)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
for i, word in index_to_word.items():
    plt.scatter(cbow_2d[i, 0], cbow_2d[i, 1], color='blue')
    plt.text(cbow_2d[i, 0] + 0.01, cbow_2d[i, 1] + 0.01, word, fontsize=9)
plt.title("CBOW Embeddings")
plt.grid(True)

plt.subplot(1, 2, 2)
for i, word in index_to_word.items():
    plt.scatter(skipgram_2d[i, 0], skipgram_2d[i, 1], color='red')
    plt.text(skipgram_2d[i, 0] + 0.01, skipgram_2d[i, 1] + 0.01, word, fontsize=9)
plt.title("Skip-gram Embeddings")
plt.grid(True)

plt.show()

from google.colab import drive
import cv2
import numpy as np
import string
import matplotlib.pyplot as plt
from imutils.object_detection import non_max_suppression
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Lambda, Bidirectional, LSTM, Dense
import os
from matplotlib.font_manager import FontProperties
font_eng = FontProperties(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf')
font_hin = FontProperties(fname='/usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf')

!pip install pyspellchecker --quiet
!pip install textblob --quiet

from spellchecker import SpellChecker
spell = SpellChecker()

def better_spell_correct_texts(texts):
    corrected = []
    for word in texts:
        # Use TextBlob for correction (handles None and empty strings)
        try:
            if word is None:
                corrected.append("")
            else:
                # TextBlob corrects empty string to empty string; str() makes sure result is string
                corrected_text = str(TextBlob(word).correct())
                corrected.append(corrected_text if corrected_text else "")
        except Exception:
            corrected.append(word if word else "")
    return corrected



# Mount Google Drive
drive.mount('/content/drive')

# Character list for recognition
char_list = string.ascii_letters + string.digits

# Path configuration
CRNN_MODEL_PATH = "/content/drive/MyDrive/OCR_Project/models/crnn_model.h5"
DEMO_IMAGE_PATH = "/content/drive/MyDrive/OCR_Project/test_images/demo_1.jpeg"
OUTPUT_DIR = "/content/drive/MyDrive/OCR_Project/outputs"

def display_intro():
    print("""
    =============================================
    Optical Character Recognition (OCR) System
    =============================================

    This system demonstrates:
    1. Text detection using EAST (OpenCV implementation)
    2. Text recognition using CRNN
    """)

def load_east_model():
    """Load the fallback OpenCV EAST model"""
    if not os.path.exists("/tmp/frozen_east_text_detection.pb"):
        east_url = "https://www.dropbox.com/s/r2ingd0l3zt8hxs/frozen_east_text_detection.tar.gz?dl=1"
        os.system("wget " + east_url + " -O /tmp/east_model.tar.gz")
        os.system("tar -xzf /tmp/east_model.tar.gz -C /tmp/")
    return cv2.dnn.readNet("/tmp/frozen_east_text_detection.pb")

def load_and_preprocess_image(image_path):
    """Load and preprocess image"""
    img = cv2.imread(image_path)
    if img is None:
        raise ValueError(f"Image not found at {image_path}")
    return img

def detect_text(image):
    """Detect text regions using EAST"""
    (h, w) = image.shape[:2]
    rW = w / float(320)
    rH = h / float(320)

    # Resize and preprocess for EAST
    blob = cv2.dnn.blobFromImage(image, 1.0, (320, 320),
                               (123.68, 116.78, 103.94), swapRB=True, crop=False)

    # Load EAST detector
    net = load_east_model()
    net.setInput(blob)
    (scores, geometry) = net.forward(["feature_fusion/Conv_7/Sigmoid",
                                    "feature_fusion/concat_3"])

    # Decode predictions
    (rects, confidences) = decode_predictions(scores, geometry)
    boxes = non_max_suppression(np.array(rects), probs=confidences)

    # Scale boxes back to original image size
    results = []
    for (startX, startY, endX, endY) in boxes:
        startX = int(startX * rW)
        startY = int(startY * rH)
        endX = int(endX * rW)
        endY = int(endY * rH)
        results.append((startX, startY, endX, endY))

    return results

def decode_predictions(scores, geometry):
    """Helper function to decode EAST predictions"""
    (numRows, numCols) = scores.shape[2:4]
    rects = []
    confidences = []

    for y in range(0, numRows):
        scoresData = scores[0, 0, y]
        xData0 = geometry[0, 0, y]
        xData1 = geometry[0, 1, y]
        xData2 = geometry[0, 2, y]
        xData3 = geometry[0, 3, y]
        anglesData = geometry[0, 4, y]

        for x in range(0, numCols):
            if scoresData[x] < 0.5:
                continue

            (offsetX, offsetY) = (x * 4.0, y * 4.0)
            angle = anglesData[x]
            cos = np.cos(angle)
            sin = np.sin(angle)

            h = xData0[x] + xData2[x]
            w = xData1[x] + xData3[x]

            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
            startX = int(endX - w)
            startY = int(endY - h)

            rects.append((startX, startY, endX, endY))
            confidences.append(scoresData[x])

    return (rects, confidences)

def preprocess_for_crnn(roi):
    """Preprocess text region for CRNN"""
    img = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    (h, w) = img.shape

    # Resize maintaining aspect ratio
    if w > 128:
        img = cv2.resize(img, (128, h))
    if h > 32:
        img = cv2.resize(img, (128, 32))

    # Pad if needed
    (h, w) = img.shape
    if h < 32:
        img = np.pad(img, [(0, 32-h), (0,0)], mode='constant', constant_values=255)
    if w < 128:
        img = np.pad(img, [(0,0), (0, 128-w)], mode='constant', constant_values=255)

    img = np.expand_dims(img, axis=-1)
    return img / 255.0

def build_crnn_model():
    """Build CRNN model architecture with proper output shapes"""
    inputs = Input(shape=(32,128,1))

    # Convolutional layers
    x = Conv2D(64, (3,3), activation='relu', padding='same')(inputs)
    x = MaxPool2D(pool_size=(2,2), strides=2)(x)

    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = MaxPool2D(pool_size=(2,2), strides=2)(x)

    x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
    x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
    x = MaxPool2D(pool_size=(2,1))(x)

    x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(pool_size=(2,1))(x)

    x = Conv2D(512, (2,2), activation='relu')(x)

    # Proper Lambda layer with output shape
    x = Lambda(lambda x: tf.squeeze(x, axis=1), output_shape=(31, 512))(x)

    # LSTM layers
    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)
    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)

    # Output layer
    outputs = Dense(len(char_list)+1, activation='softmax')(x)

    return Model(inputs, outputs)

def load_crnn_model():
    """Load CRNN model with proper architecture"""
    try:
        model = build_crnn_model()
        model.load_weights(CRNN_MODEL_PATH)
        return model
    except Exception as e:
        print(f"Error loading CRNN model: {str(e)}")
        return None

def recognize_text(model, cropped_images):
    """Recognize text using the model"""
    if model is None:
        return [""] * len(cropped_images)

    # Prepare batch
    batch = np.array([preprocess_for_crnn(img) for img in cropped_images])

    try:
        # Predict
        preds = model.predict(batch, verbose=0)

        # Decode predictions
        results = []
        input_length = np.ones(preds.shape[0]) * preds.shape[1]

        # Process each prediction individually to avoid shape issues
        for i in range(preds.shape[0]):
            pred = np.expand_dims(preds[i], axis=0)
            decoded = tf.keras.backend.ctc_decode(
                pred,
                input_length=np.array([pred.shape[1]]),
                greedy=True
            )[0][0].numpy()

            text = ''.join([char_list[int(x)] for x in decoded[0] if int(x) != -1])
            results.append(text)

        return results
    except Exception as e:
        print(f"Error during recognition: {str(e)}")
        return [""] * len(cropped_images)

def visualize_results(image, boxes, texts):
    """Display and save results"""
    output = image.copy()

    # Draw bounding boxes and text
    for i, (box, text) in enumerate(zip(boxes, texts)):
        (startX, startY, endX, endY) = box
        cv2.rectangle(output, (startX, startY), (endX, endY), (0, 255, 0), 2)

        # Add recognized text
        if text:
            cv2.putText(output, text, (startX, startY-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        else:
            cv2.putText(output, f"Region {i+1}", (startX, startY-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

    # Display
    plt.figure(figsize=(15, 10))
    plt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    # Save
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    cv2.imwrite(os.path.join(OUTPUT_DIR, "final_output.jpg"), output)

def main():
    display_intro()

    try:
        # Verify files
        if not os.path.exists(DEMO_IMAGE_PATH):
            raise FileNotFoundError(f"Demo image not found at {DEMO_IMAGE_PATH}")

        # Load image
        print("Loading image...")
        image = load_and_preprocess_image(DEMO_IMAGE_PATH)

        # Detect text
        print("Detecting text regions...")
        boxes = detect_text(image)
        print(f"Found {len(boxes)} text regions")

        # Crop detected regions
        cropped_images = [image[startY:endY, startX:endX] for (startX, startY, endX, endY) in boxes]

        # Load CRNN model
        print("Loading text recognizer...")
        crnn_model = load_crnn_model()

        # Recognize text
        print("Recognizing text...")
        recognized_texts = recognize_text(crnn_model, cropped_images)

        # ADD SPELL CORRECTION RIGHT HERE!
        corrected_texts = better_spell_correct_texts(recognized_texts)
        valid_texts = [text for text in corrected_texts if text.strip()]
        print(f"‚úÖ Spell-corrected {len(valid_texts)} non-empty recognized text regions")

        # Now use valid_texts as your input for Phase 4
        # For NLP translation/summarization downstream
        # translations = translator_module.translate_texts(valid_texts)
        # summary = summarizer_module.summarize_texts(valid_texts)
        # etc...

        # Display results
        visualize_results(image, boxes, recognized_texts)

        print("\nProcessing complete! Results saved in:", OUTPUT_DIR)
        print("\nText regions with recognized text:")
        for i, ((startX, startY, endX, endY), text) in enumerate(zip(boxes, recognized_texts)):
            print(f"Region {i+1}: ({startX}, {startY}) to ({endX}, {endY}) - {text if text else 'Not recognized'}")

    except Exception as e:
        print(f"\nError: {str(e)}")
        print("\nTroubleshooting steps:")
        print("1. Check all file paths are correct")
        print("2. Verify your image file exists")
        print("3. Ensure you have internet access for EAST model download")

if __name__ == "__main__":
    main()

# PHASE-4: NLP MODULE SETUP
print("="*50)
print("PHASE-4: NLP Translation & Summarization Module")
print("="*50)

# Install required packages
!pip install transformers torch sentencepiece sacremoses datasets evaluate rouge-score nltk --quiet

import torch
from transformers import pipeline, MarianMTModel, MarianTokenizer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import warnings
warnings.filterwarnings('ignore')

!apt-get -qq install fonts-noto fonts-dejavu
!apt-get -qq install fonts-noto

import matplotlib
from matplotlib import pyplot as plt
from matplotlib.font_manager import FontProperties
matplotlib.rcParams['font.family'] = 'DejaVu Sans'

print("Font family set to:", matplotlib.rcParams['font.family'])


print("‚úÖ Dependencies installed successfully!")

class TranslationModule:
    def __init__(self, src_lang='en', tgt_lang='hi'):
        """Initialize translation module with language pair"""
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.model_name = f"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}"

        print(f"Loading translation model: {self.model_name}")
        try:
            self.translator = pipeline(
                "translation",
                model=self.model_name,
                tokenizer=self.model_name,
                device=0 if torch.cuda.is_available() else -1
            )
            print("‚úÖ Translation model loaded successfully!")
        except Exception as e:
            print(f"‚ùå Error loading translation model: {e}")
            # Fallback to a general multilingual model
            self.translator = pipeline(
                "translation",
                model="facebook/mbart-large-50-many-to-many-mmt",
                device=0 if torch.cuda.is_available() else -1
            )
            print("‚úÖ Fallback multilingual model loaded!")

    def translate_texts(self, texts):
      """Translate list of texts"""
      translations = []
      for text in texts:
        if text.strip():  # Only translate non-empty texts
            try:
                # Handle text length limits
                if len(text) > 500:
                    text = text[:500]  # Truncate long texts
                # Attempt translation
                result = self.translator(text)
                # The translation pipeline may return [] for blanks, so check!
                if isinstance(result, list) and len(result) > 0:
                    translated = result[0]['translation_text']
                elif isinstance(result, dict) and "translation_text" in result:
                    translated = result['translation_text']
                else:
                    translated = ""
                translations.append(translated)
            except Exception as e:
                print(f"Translation error for '{text[:50]}...': {e}")
                translations.append("")  # Always append something!
        else:
            translations.append("")  # Blank if input is blank
      return translations


# Initialize translation module
translator_module = TranslationModule(src_lang='en', tgt_lang='hi')  # English to Hindi

class SummarizationModule:
    def __init__(self, model_name="facebook/bart-large-cnn"):
        """Initialize summarization module"""
        self.model_name = model_name

        print(f"Loading summarization model: {model_name}")
        try:
            self.summarizer = pipeline(
                "summarization",
                model=model_name,
                tokenizer=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
            print("‚úÖ Summarization model loaded successfully!")
        except Exception as e:
            print(f"‚ùå Error loading summarization model: {e}")
            # Fallback to T5
            self.summarizer = pipeline(
                "summarization",
                model="t5-small",
                tokenizer="t5-small",
                device=0 if torch.cuda.is_available() else -1
            )
            print("‚úÖ Fallback T5 model loaded!")

    def summarize_texts(self, texts, max_length=100, min_length=20):
        """Summarize concatenated texts or individual texts"""
        if not texts or all(not text.strip() for text in texts):
            return ""

        # Combine all texts for document-level summarization
        combined_text = " ".join([text.strip() for text in texts if text.strip()])

        if len(combined_text) < min_length:
            return combined_text  # Too short to summarize

        try:
            # Handle input length limits (BART limit ‚âà 1024 tokens)
            if len(combined_text) > 3000:  # Approximate token limit
                combined_text = combined_text[:3000]

            summary = self.summarizer(
                combined_text,
                max_length=max_length,
                min_length=min_length,
                do_sample=False
            )

            return summary[0]['summary_text']
        except Exception as e:
            print(f"Summarization error: {e}")
            return combined_text[:200] + "..."  # Fallback truncation

# Initialize summarization module
summarizer_module = SummarizationModule()

def enhanced_ocr_pipeline(image_path):
    """Complete OCR pipeline with Phase-4 NLP processing"""

    print("\n" + "="*60)
    print("ENHANCED OCR PIPELINE WITH PHASE-4 NLP")
    print("="*60)

    try:
        # PHASES 1-3: Existing OCR Process
        print("\nüì∏ PHASE 1-3: Text Detection & Recognition")

        # Load and process image
        image = load_and_preprocess_image(image_path)

        # Detect text regions
        boxes = detect_text(image)
        print(f"‚úÖ Found {len(boxes)} text regions")

        # Crop and recognize text
        cropped_images = [image[startY:endY, startX:endX] for (startX, startY, endX, endY) in boxes]
        crnn_model = load_crnn_model()
        # Recognize text
        recognized_texts = recognize_text(crnn_model, cropped_images)
        print(f"‚úÖ Successfully recognized {len(recognized_texts)} text regions")

        # Spell correction on all recognized texts to keep counts aligned
        corrected_texts = better_spell_correct_texts(recognized_texts)
        print(f"‚úÖ Spell-corrected {len(corrected_texts)} text regions")

        # PHASE 4: NLP Processing
        print("\nü§ñ PHASE 4: NLP Translation & Summarization")

        print("üîÑ Translating texts...")
        # TRANSLATE THE ENTIRE LIST (including empty lines for perfect side-by-side alignment)
        translations = translator_module.translate_texts(corrected_texts)

        print("üìù Generating summary...")
        # You can keep summary only over valid non-empty text if you prefer, or all text:
        summary = summarizer_module.summarize_texts([t for t in corrected_texts if t.strip()])

        # Results visualization
        visualize_enhanced_results(image, boxes, corrected_texts, translations, summary)

        return {
            'boxes': boxes,
            'original_texts': corrected_texts,
            'translations': translations,
            'summary': summary,
            'image': image
        }

    except Exception as e:
        print(f"‚ùå Pipeline error: {str(e)}")
        return None


def is_hindi(text):
    """Detect if text contains any Devanagari character"""
    for ch in text:
        if '\u0900' <= ch <= '\u097F':
            return True
    return False

def visualize_enhanced_results(image, boxes, original_texts, translations, summary):
    # existing image + box drawing code remains same...

    # Create the combined text for display
    results_lines = []

    results_lines.append("PHASE-4 NLP RESULTS")
    results_lines.append("=" * 50)
    results_lines.append("")
    results_lines.append("üîé ORIGINAL TEXTS:")
    for i, text in enumerate(original_texts):
        results_lines.append(f"{i + 1}. {text}")
    results_lines.append("")
    results_lines.append("üåê TRANSLATIONS (English ‚Üí Hindi):")
    for i, trans in enumerate(translations):
        results_lines.append(f"{i + 1}. {trans}")
    results_lines.append("")
    results_lines.append("üìù DOCUMENT SUMMARY:")
    results_lines.append(summary)

    # Plot setup
    plt.figure(figsize=(20, 15))

    # Show the main image
    plt.subplot(2, 2, 1)
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title("Text Detection & Recognition", fontsize=14, fontweight='bold')
    plt.axis('off')

    # Result text box
    plt.subplot(2, 2, (2, 4))
    plt.axis('off')

    # Starting Y position in axes coordinates
    y = 0.95
    line_height = 0.03

    for line in results_lines:
        if is_hindi(line):
            font = font_hin
        else:
            font = font_eng

        plt.text(0.01, y, line, fontsize=12, fontproperties=font, verticalalignment='top')
        y -= line_height
        if y < 0:
            break  # Avoid overflow

    plt.tight_layout()
    plt.show()

    # Save results
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # Save enhanced output image
    cv2.imwrite(os.path.join(OUTPUT_DIR, "phase4_enhanced_output.jpg"), image)

    # Save text results
    with open(os.path.join(OUTPUT_DIR, "phase4_results.txt"), "w", encoding="utf-8") as f:
        f.write('\n'.join(results_lines))

    print(f"‚úÖ Enhanced results saved to: {OUTPUT_DIR}")

# RUN ENHANCED OCR PIPELINE WITH PHASE-4
def run_enhanced_demo():
    """Run the complete enhanced OCR pipeline"""

    print("\nüöÄ Starting Enhanced OCR Pipeline Demo...")

    # Run enhanced pipeline
    results = enhanced_ocr_pipeline(DEMO_IMAGE_PATH)

    if results:
        print("\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
        print("\nüìä Final Results Summary:")
        print(f"‚Ä¢ Text regions detected: {len(results['boxes'])}")
        print(f"‚Ä¢ Texts recognized: {len([t for t in results['original_texts'] if t.strip()])}")
        print(f"‚Ä¢ Translations generated: {len(results['translations'])}")
        print(f"‚Ä¢ Summary: {results['summary'][:100]}..." if len(results['summary']) > 100 else f"‚Ä¢ Summary: {results['summary']}")

        # Performance metrics
        print(f"\n‚ö° Performance Info:")
        print(f"‚Ä¢ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        print(f"‚Ä¢ Models loaded: Translation ‚úÖ, Summarization ‚úÖ")

    else:
        print("‚ùå Pipeline failed. Check error messages above.")

# Run the demo
run_enhanced_demo()

# INTERACTIVE TESTING AND EVALUATION
def test_individual_modules():
    """Test individual NLP modules"""

    print("\nüß™ TESTING INDIVIDUAL MODULES")
    print("="*40)

    # Sample texts for testing
    test_texts = [
        "Hello world, this is a test.",
        "Machine learning is transforming technology.",
        "Vision transformers are powerful models."
    ]

    print("üî§ Original Texts:")
    for i, text in enumerate(test_texts):
        print(f"  {i+1}. {text}")

    # Test translation
    print("\nüåê Testing Translation...")
    translations = translator_module.translate_texts(test_texts)
    for i, trans in enumerate(translations):
        print(f"  {i+1}. {trans}")

    # Test summarization
    print("\nüìù Testing Summarization...")
    summary = summarizer_module.summarize_texts(test_texts)
    print(f"  Summary: {summary}")

    # Language support info
    print(f"\nüåç Current Language Configuration:")
    print(f"  ‚Ä¢ Source: {translator_module.src_lang}")
    print(f"  ‚Ä¢ Target: {translator_module.tgt_lang}")
    print(f"  ‚Ä¢ Model: {translator_module.model_name}")

def change_language_pair(src_lang, tgt_lang):
    """Change translation language pair"""
    global translator_module
    translator_module = TranslationModule(src_lang=src_lang, tgt_lang=tgt_lang)
    print(f"‚úÖ Language pair changed to: {src_lang} ‚Üí {tgt_lang}")

# Run tests
test_individual_modules()

# Example: Change to different language pair
# change_language_pair('en', 'fr')  # English to French
# change_language_pair('en', 'de')  # English to German
